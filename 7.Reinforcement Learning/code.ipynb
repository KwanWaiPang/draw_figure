{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./lunar_lander.gif\" width = 60% >\n",
    "</div>\n",
    "\n",
    "* 选用kernel learning_dl\n",
    "~~~\n",
    "conda activate learning_dl \n",
    "\n",
    "pip install gym\n",
    "pip install imageio\n",
    "pip install statsmodels\n",
    "pip install pyvirtualdisplay\n",
    "\n",
    "sudo apt-get install xvfb\n",
    "pip install xvfbwrapper\n",
    "\n",
    "pip install gym[box2d]\n",
    "\n",
    "# conda remove --name learning_dl --all\n",
    "# kill -9 +id\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwp/miniconda3/envs/learning_dl/lib/python3.6/site-packages/gym/core.py:27: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish import module!!!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(\"finish import module!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a virtual display to render the Lunar Lander environment.\n",
    "Display(visible=0, size=(840, 480)).start();\n",
    "\n",
    "# Set the random seed for TensorFlow\n",
    "tf.random.set_seed(utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置超参\n",
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载环境\n",
    "# env = gym.make('LunarLander-v2')\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x7F684900DF98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # reset the environment加载环境后将其重置\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (8,)\n",
      "Number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)\n",
    "# 获取环境的状态空间和动作空间的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: (array([-0.007, 1.410, -0.729, -0.040, 0.008, 0.165, 0.000, 0.000],\n",
      "      dtype=float32), {})\n",
      "Action: 0\n",
      "Next State: [-0.014 1.409 -0.728 -0.065 0.017 0.163 0.000 0.000]\n",
      "Reward Received: -0.7608696464783975\n",
      "Episode Terminated: False\n",
      "Info: False\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment and get the initial state.\n",
    "initial_state = env.reset()\n",
    "\n",
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "# print(env.step(action))\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, info,_ = env.step(action)\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.3f}'.format}):\n",
    "    print(\"Initial State:\", initial_state)\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Next State:\", next_state)\n",
    "    print(\"Reward Received:\", reward)\n",
    "    print(\"Episode Terminated:\", done)\n",
    "    print(\"Info:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义神经网络来拟合Q-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n",
      "\u001b[92mAll tests passed!\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# UNQ_C1\n",
    "# GRADED CELL\n",
    "\n",
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    ### START CODE HERE ### \n",
    "    Input(shape=state_size),                      \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=num_actions, activation='linear'),\n",
    "    ### END CODE HERE ### \n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    ### START CODE HERE ### \n",
    "    Input(shape=state_size),                       \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=num_actions, activation='linear'), \n",
    "    ### END CODE HERE ###\n",
    "    ])\n",
    "\n",
    "### START CODE HERE ### \n",
    "optimizer = Adam(learning_rate=ALPHA) #None\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# UNIT TEST\n",
    "from public_tests import *\n",
    "\n",
    "test_network(q_network)\n",
    "test_network(target_q_network)\n",
    "test_optimizer(optimizer, ALPHA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When an agent interacts with the environment, the states, actions, and rewards the agent experiences are sequential by nature. \n",
    "# If the agent tries to learn from these consecutive experiences it can run into problems due to the strong correlations between them. \n",
    "# To avoid this, we employ a technique known as Experience Replay to generate uncorrelated experiences for training our agent. \n",
    "# Experience replay consists of storing the agent's experiences (i.e the states, actions, and rewards the agent receives) in a memory buffer and then sampling a random mini-batch of experiences from the buffer to do the learning. \n",
    "# The experience tuples will be added to the memory buffer at each time step as the agent interacts with the environment.\n",
    "\n",
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "#计算loss\n",
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "# UNQ_C2\n",
    "# GRADED FUNCTION: calculate_loss\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Karas model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    ### START CODE HERE ### \n",
    "    y_targets =  rewards + (gamma * max_qsa * (1 - done_vals)) #None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get the q_values\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the loss\n",
    "    ### START CODE HERE ### \n",
    "    loss = MSE(y_targets, q_values) #None \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return loss\n",
    "\n",
    "# UNIT TEST    \n",
    "test_compute_loss(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新网络的权重\n",
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    utils.update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -149.23\n",
      "Episode 200 | Total point average of the last 100 episodes: -99.737\n",
      "Episode 249 | Total point average of the last 100 episodes: -78.997"
     ]
    }
   ],
   "source": [
    "# 进行训练\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state,_ = env.reset()\n",
    "    total_points = 0\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        \n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        state_qn = np.expand_dims(state, axis=0)  # state needs to be the right shape for the q_network\n",
    "        state_qn = tf.convert_to_tensor(state_qn, dtype=tf.float32)  # 强制转换为TensorFlow张量\n",
    "        q_values = q_network(state_qn)\n",
    "        action = utils.get_action(q_values, epsilon)\n",
    "        \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward, done, _,_ = env.step(action)\n",
    "        \n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the done variable as well for convenience.\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = utils.check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "            experiences = utils.get_experiences(memory_buffer)\n",
    "            \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    # Update the ε value\n",
    "    epsilon = utils.get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if av_latest_points >= 200.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('lunar_lander_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
